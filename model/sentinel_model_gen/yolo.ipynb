{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc7bd53",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f5545",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "KAGGLE_ENVIRONMENT = False\n",
    "\n",
    "if os.path.isdir(\"/kaggle/working\"):\n",
    "    KAGGLE_ENVIRONMENT = True\n",
    "    \n",
    "    import subprocess\n",
    "    \n",
    "    print(\"In Kaggle environment\")\n",
    "    \n",
    "    # If we don't uninstall `wandb`, `ultralytics` will keep asking for an API key.\n",
    "    print(\"Uninstalling `wandb`...\")\n",
    "    completed = subprocess.run([\"pip\", \"uninstall\", \"wandb\", \"-y\"], capture_output=True)\n",
    "    if completed.returncode == 0:\n",
    "        print(\"Successfully uninstalled `wandb`\")\n",
    "    else:\n",
    "        print(f\"Failed to uninstall `wandb`: {completed.stderr}\")\n",
    "        \n",
    "    for pkg in (\"roboflow\", \"fiftyone\", \"ultralytics\"):\n",
    "        print(f\"Installing `{pkg}`...\")\n",
    "        completed = subprocess.run([\"pip\", \"install\", pkg], capture_output=True)\n",
    "        if completed.returncode == 0:\n",
    "            print(f\"Successfully installed `{pkg}`\")\n",
    "        else:\n",
    "            print(f\"Failed to install `{pkg}`: {completed.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673d8c8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.random as four\n",
    "import matplotlib.pyplot as plt\n",
    "import pymongo\n",
    "import yaml\n",
    "from ray import tune\n",
    "from roboflow import Roboflow\n",
    "from roboflow.core.dataset import Dataset as RoboflowDataset\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c9ffc",
   "metadata": {},
   "source": [
    "### General Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder paths.\n",
    "ROOT_PATH = \"/kaggle/working/root\" if KAGGLE_ENVIRONMENT else os.path.abspath(os.getcwd())\n",
    "\n",
    "IMAGES_PATH = os.path.join(ROOT_PATH, 'images')\n",
    "\n",
    "# Name of the directory for combining datasets.\n",
    "COMBINED_DATASET_DIR = \"combined\"\n",
    "\n",
    "# Format to use when downloading Roboflow datasets.\n",
    "RF_DATASET_FORMAT = \"yolov8\"\n",
    "\n",
    "# Format to use when downloading FiftyOne datasets.\n",
    "FO_DATASET_FORMAT = fo.types.YOLOv5Dataset # YOLOv5 and YOLOv8 use the same format\n",
    "\n",
    "# Output path when exporting the model.\n",
    "MODEL_OUTPUT_PATH = os.path.join(ROOT_PATH, \"sentinel_default_v2.pt\") # changed the model name for v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5172f73",
   "metadata": {},
   "source": [
    "FiftyOne uses MongoDB to manage its datasets. When possible, FiftyOne will automatically set up the database for you. However, when it fails to do so, you need to manually set up a MongoDB database. The code below checks if FiftyOne is able to set up the database — if not, then you must set up your own and specify the connection string. After installing MongoDB, run `mongod --dbpath <DBPATH>`, replacing `DBPATH` with any path of your choice. By default (no authentication and using the default port), the connection string is: `mongodb://localhost:27017`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        print(\"Trying to reach MongoDB...\")\n",
    "        fo.core.odm.database.get_db_config()\n",
    "        print(\"MongoDB is reachable.\")\n",
    "        break\n",
    "    except (fo.core.config.FiftyOneConfigError, pymongo.errors.ServerSelectionTimeoutError):\n",
    "        print(\"Failed to reach a running MongoDB instance. Enter a valid MongoDB connection string:\")\n",
    "        db_uri = input()\n",
    "        fo.config.database_uri = db_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8555d144",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bd25e",
   "metadata": {},
   "source": [
    "Helper function to `gitignore` a directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gitignore(directory: str):\n",
    "    \"\"\"\n",
    "    Make the given directory ignored by Git.\n",
    "\n",
    "    No prefixes are prepended to the directory. The directory must already exist.\n",
    "\n",
    "    This function adds a `.gitignore` file to the directory\n",
    "    containing the wildcard pattern \"*\" so that git ignores it.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(directory):\n",
    "        raise ValueError(\"The given path does not exist or is not a directory.\")\n",
    "        \n",
    "    gitignore_path = os.path.join(directory, \".gitignore\")\n",
    "    with open(gitignore_path, \"w\") as gitignore_file:\n",
    "        gitignore_file.write(\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2595d5",
   "metadata": {},
   "source": [
    "We'll create the `IMAGES_PATH` directory early to make `git` ignore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(IMAGES_PATH):\n",
    "    os.makedirs(IMAGES_PATH)\n",
    "    print(f\"Created '{IMAGES_PATH}' directory.\")\n",
    "else:\n",
    "    print(f\"'{IMAGES_PATH}' exists — nothing to do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(IMAGES_PATH, \".gitignore\")):\n",
    "    gitignore(IMAGES_PATH)\n",
    "    print(f\"Gitignored '{IMAGES_PATH}'.\")\n",
    "else:\n",
    "    print(f\"'{IMAGES_PATH}/.gitignore' exists — skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3241b85",
   "metadata": {},
   "source": [
    "### Roboflow Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e487aa95",
   "metadata": {},
   "source": [
    "To download datasets from Roboflow, you must have a Roboflow API key. This notebook will attempt to load the API key from the `ROBOFLOW_API_KEY` environment variable or Kaggle's secrets management. If the variable does not exist, then you will be prompted for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2085998",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_api_key: str | None = None\n",
    "\n",
    "if \"ROBOFLOW_API_KEY\" in os.environ:\n",
    "    rf_api_key = os.environ[\"ROBOFLOW_API_KEY\"]\n",
    "elif KAGGLE_ENVIRONMENT:\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        kaggle_user_secrets = UserSecretsClient()\n",
    "        rf_api_key = kaggle_user_secrets.get_secret(\"ROBOFLOW_API_KEY\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Failed to retrieve Roboflow API key from Kaggle: {repr(ex)}\")\n",
    "\n",
    "if rf_api_key is None:\n",
    "    print(\"Could not find Roboflow API key.\")\n",
    "    print(\"Please enter your Roboflow API key: \")\n",
    "    rf_api_key = input()\n",
    "\n",
    "rf = Roboflow(api_key=rf_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_roboflow_dataset(workspace: str, project: str, version: str, directory: str, dataset_format=RF_DATASET_FORMAT):\n",
    "    \"\"\"\n",
    "    Downloads the specified Roboflow dataset into the given directory\n",
    "    and returns the dataset as a Roboflow `Dataset` object.\n",
    "\n",
    "    The directory will be prefixed by `IMAGES_PATH`.\n",
    "\n",
    "    If the directory already exists, the dataset will not be redownloaded.\n",
    "    \"\"\"\n",
    "    abs_directory = os.path.join(IMAGES_PATH, directory)\n",
    "\n",
    "    rf_project = rf.workspace(workspace).project(project)\n",
    "    rf_version = rf_project.version(version)\n",
    "    \n",
    "    if os.path.exists(abs_directory):\n",
    "        print(f\"Path '{abs_directory}' exists — refusing to overwrite.\")\n",
    "        print(\"If you want to redownload the dataset, please manually remove the directory.\")\n",
    "        return RoboflowDataset(rf_version.name, rf_version.version, dataset_format, abs_directory)\n",
    "        \n",
    "    dataset = rf_version.download(dataset_format, location=abs_directory)\n",
    "\n",
    "    print(f\"Dataset downloaded to: {abs_directory}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_ds = download_roboflow_dataset(\"liteye-systems\", \"weapon-classification\", \"2\", \"guns\")\n",
    "knife_ds = download_roboflow_dataset(\"knife-detection-sjzqp\", \"knife-detection-bstjz\", \"2\", \"knife\") # new knife dataset\n",
    "parcel_ds = download_roboflow_dataset(\"king-mongkuts-institute-of-technology-ladkrabang-vaztb\", \"package-detection-hfpr9\", \"4\", \"parcel\") # new parcel dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3c274",
   "metadata": {},
   "source": [
    "Unfortunately, due to a [Roboflow bug](https://github.com/roboflow/roboflow-python/issues/240), the paths in the YAML file are wrong, so we'll manually fix them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataset_yaml_paths(yaml_path: str, train_rel_path: str, valid_rel_path: str, test_rel_path: str):\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        yaml_content = yaml.safe_load(file)\n",
    "\n",
    "    yaml_content[\"train\"] = train_rel_path\n",
    "    yaml_content[\"val\"] = valid_rel_path\n",
    "    yaml_content[\"test\"] = test_rel_path\n",
    "\n",
    "    with open(yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_content, file)\n",
    "        print(f\"Updated '{yaml_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eac11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_dataset_yaml_paths(\n",
    "    os.path.join(IMAGES_PATH, \"guns\", \"data.yaml\"),\n",
    "    train_rel_path=\"./train/images\",\n",
    "    valid_rel_path=\"./valid/images\",\n",
    "    test_rel_path=\"./test/images\",\n",
    ")\n",
    "\n",
    "# New knife dataset\n",
    "fix_dataset_yaml_paths(\n",
    "    os.path.join(IMAGES_PATH, \"knife\", \"data.yaml\"),\n",
    "    train_rel_path=\"./train/images\",\n",
    "    valid_rel_path=\"./valid/images\",\n",
    "    test_rel_path=\"./test/images\",\n",
    ")\n",
    "\n",
    "# New parcel dataset \n",
    "fix_dataset_yaml_paths(\n",
    "    os.path.join(IMAGES_PATH, \"parcel\", \"data.yaml\"),\n",
    "    train_rel_path=\"./train/images\",\n",
    "    valid_rel_path=\"./valid/images\",\n",
    "    test_rel_path=\"./test/images\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6868ab",
   "metadata": {},
   "source": [
    "### COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4faf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_coco2017(\n",
    "    categories: Optional[list[str]] = [\"person\"],\n",
    "    max_samples: Optional[int] = None,\n",
    "    directory: str = \"coco-2017\",\n",
    "    dataset_format=FO_DATASET_FORMAT,\n",
    "    seed: int = 0,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads the COCO 2017 dataset into the given directory.\n",
    "\n",
    "    All splits will be downloaded. The dataset can be filtered by category\n",
    "    using the `categories` argument. If `max_samples` is specified, then each\n",
    "    split will be limited to have a maximum of `max_samples` number of samples.\n",
    "    \n",
    "    By default, the dataset will be exported in the format specified by `FO_DATASET_FORMAT`.\n",
    "    To change the output format, specify the `dataset_format` argument.\n",
    "    \"\"\"\n",
    "    # Unfortunately, the test split of COCO 2017 does not have labels,\n",
    "    # which makes it not so useful here.\n",
    "    splits = [\"train\", \"validation\"]\n",
    "    \n",
    "    dataset = fo.zoo.load_zoo_dataset(\n",
    "        \"coco-2017\",\n",
    "        splits=splits,\n",
    "        label_types=[\"detections\"],\n",
    "        max_samples=max_samples,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Rename 'validation' split to 'val'\n",
    "    validation_view = dataset.match_tags(\"validation\")\n",
    "    validation_view.tag_samples(\"val\")\n",
    "    validation_view.untag_samples(\"validation\")\n",
    "\n",
    "    splits.remove(\"validation\")\n",
    "    splits.append(\"val\")\n",
    "\n",
    "    # Use half of the validation set as a test set.\n",
    "    # Note that we are not explicitly tagging the test samples;\n",
    "    # otherwise, multiple calls to this function will repeatedly shrink the validation set.\n",
    "    validation_view = dataset.match_tags(\"val\")\n",
    "    validation_view, test_view = four.random_split(validation_view, [0.5, 0.5], seed=seed)\n",
    "    train_view = dataset.match_tags(\"train\")\n",
    "\n",
    "    print(train_view.stats())\n",
    "    print(validation_view.stats())\n",
    "    print(test_view.stats())\n",
    "\n",
    "    ds_view = dataset.view()\n",
    "\n",
    "    # Manually filter the dataset to samples matching the given catgories\n",
    "    # due to a bug: https://github.com/voxel51/fiftyone/issues/4570\n",
    "    # Workaround based on: https://github.com/voxel51/fiftyone/issues/4570#issuecomment-2392548410\n",
    "    # Unfortunately, the workaround downloads images we don't need and then filters them,\n",
    "    # so we waste a bit of space and network bandwidth.\n",
    "    if categories is not None:\n",
    "        ds_view = ds_view.filter_labels(\"ground_truth\", fo.ViewField(\"label\").is_in(categories))\n",
    "\n",
    "    # Export in YOLOv8 format.\n",
    "    # According to https://github.com/voxel51/fiftyone/issues/3392#issuecomment-1666520356,\n",
    "    # splits must be exported separately.\n",
    "    export_dir = os.path.join(IMAGES_PATH, directory)\n",
    "    for split, view in {\n",
    "        \"train\": train_view,\n",
    "        \"val\": validation_view,\n",
    "        \"test\": test_view\n",
    "    }.items():\n",
    "        view.export(\n",
    "            export_dir=export_dir,\n",
    "            dataset_type=dataset_format,\n",
    "            split=split,\n",
    "            classes=categories,\n",
    "        )\n",
    "        print(f\"Split '{split}' exported to '{export_dir}/{split}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_coco2017(max_samples=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df0c6a",
   "metadata": {},
   "source": [
    "### Combining the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f736a6",
   "metadata": {},
   "source": [
    "#### Update Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0474574",
   "metadata": {},
   "source": [
    "Before we can combine the datasets, we first need to update the labels for each dataset to use global indices. Otherwise, different datasets will use the same index for different classes. We'll first construct a map to keep track of the mapping between indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29be845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_index_map(dirs: list[tuple[str, str]], images_path: str = IMAGES_PATH) -> dict[str, dict[int | str, int]]:\n",
    "    \"\"\"\n",
    "    Constructs a dictionary that maps the index and name of each category of the given datasets\n",
    "    to a global index in preparation for combining them.\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    index_map: dict[str, dict[int, int]] = {}\n",
    "    for ds_dir, ds_yaml in dirs:\n",
    "        yaml_path = os.path.join(images_path, ds_dir, ds_yaml)\n",
    "        index_map[ds_dir] = {}\n",
    "        \n",
    "        with open(yaml_path, \"r\") as file:\n",
    "            yaml_content = yaml.safe_load(file)\n",
    "            \n",
    "        names = yaml_content[\"names\"]\n",
    "        if type(names) == dict:\n",
    "            it = names.items()\n",
    "        elif type(names) == list:\n",
    "            it = enumerate(names)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown type for 'names'.\")\n",
    "\n",
    "        for idx, name in it:\n",
    "            index_map[ds_dir][idx] = index\n",
    "            index_map[ds_dir][name] = index\n",
    "            index += 1\n",
    "            \n",
    "    return index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60560a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = construct_index_map([\n",
    "    (\"coco-2017\", \"dataset.yaml\"),\n",
    "    (\"guns\", \"data.yaml\"),\n",
    "    (\"knife\", \"data.yaml\"), # new knife dataset\n",
    "    (\"parcel\", \"data.yaml\"), # new parcel dataset \n",
    "])\n",
    "index_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90825b3f",
   "metadata": {},
   "source": [
    "Now, we update the labels in each dataset's YAML file and `labels` directory (or directories):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_labels_yaml(\n",
    "    ds_dir: str,\n",
    "    yaml_rel_path: str,\n",
    "    yaml_content: str,\n",
    "    index_map: dict[str, dict[int, int]],\n",
    "    images_path: str = IMAGES_PATH\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the labels in the given dataset's YAML file to use the global indices in `index_map`.\n",
    "\n",
    "    Do not call this function directly! Call `update_labels()` instead to make sure the dataset remains consistent.\n",
    "    \"\"\"\n",
    "    names = yaml_content[\"names\"]\n",
    "    if type(names) == dict:\n",
    "        name_strings = names.values()\n",
    "    elif type(names) == list:\n",
    "        name_strings = names\n",
    "    else:\n",
    "        raise ValueError(\"Unknown type for 'names'.\")\n",
    "        \n",
    "    new_yaml_names = {}\n",
    "    for name in name_strings:\n",
    "        index = index_map[ds_dir][name]\n",
    "        new_yaml_names[index] = name\n",
    "\n",
    "    yaml_content[\"names\"] = new_yaml_names\n",
    "\n",
    "    yaml_path = os.path.join(IMAGES_PATH, ds_dir, yaml_rel_path)\n",
    "    with open(yaml_path, \"w\") as yaml_file:\n",
    "        yaml.dump(yaml_content, yaml_file)\n",
    "\n",
    "    print(f\"Updated '{yaml_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_labels_txt(\n",
    "    ds_dir: str,\n",
    "    yaml_rel_path: str,\n",
    "    yaml_content: str,\n",
    "    index_map: dict[str, dict[int, int]],\n",
    "    images_path: str = IMAGES_PATH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the labels in the given dataset's label files to use the global indices in `index_map`.\n",
    "    \n",
    "    Do not call this function directly! Call `update_labels()` instead to make sure the dataset remains consistent.\n",
    "    \"\"\"\n",
    "    yaml_path = os.path.join(IMAGES_PATH, ds_dir, yaml_rel_path)\n",
    "    for unresolved_img_dir in (yaml_content[\"train\"], yaml_content[\"val\"], yaml_content[\"test\"]):\n",
    "        if os.path.isabs(unresolved_img_dir):\n",
    "            # Just use the directory as is.\n",
    "            img_dir = unresolved_img_dir\n",
    "        else: # Relative path\n",
    "            parent_yaml_path = os.path.dirname(yaml_path) # Get parent directory of the YAML file.\n",
    "            img_dir = os.path.join(parent_yaml_path, unresolved_img_dir) # Append the directory to the parent.\n",
    "            img_dir = os.path.abspath(img_dir) # Normalise.\n",
    "\n",
    "        img_dir = Path(img_dir)\n",
    "\n",
    "        # Find the last occurrence of \"images\" in the path.\n",
    "        # The way the YOLO format works to find the labels directory is to replace\n",
    "        # the last occurrence of \"images\" in the path with \"labels\".\n",
    "        img_dir_parts = list(img_dir.parts)\n",
    "        last_images_idx = len(img_dir_parts) - 1 - img_dir_parts[::-1].index(\"images\")\n",
    "        assert img_dir.parts[last_images_idx] == \"images\"\n",
    "\n",
    "        # Replace the last \"images\" with \"labels\" and reconstruct the path.\n",
    "        img_dir_parts[last_images_idx] = \"labels\"\n",
    "        labels_dir = Path(*img_dir_parts)\n",
    "\n",
    "        # Iterate over the files in the labels directory.\n",
    "        for label_file_path in labels_dir.iterdir():\n",
    "            if not label_file_path.is_file():\n",
    "                print(f\"'{label_file_path}' is not a file — skipping\")\n",
    "                continue\n",
    "\n",
    "            # Read label file contents.\n",
    "            with label_file_path.open(\"r\") as label_file:\n",
    "                label_contents = label_file.readlines()\n",
    "\n",
    "            # Modify label file class IDs\n",
    "            with open(label_file_path, \"w\") as label_file:\n",
    "                for line in label_contents:\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0])\n",
    "                    new_class_id = index_map[ds_dir][class_id]\n",
    "                    new_line = f\"{new_class_id} \" + \" \".join(parts[1:]) + \"\\n\"\n",
    "                    label_file.write(new_line)\n",
    "\n",
    "        print(f\"Updated labels in '{labels_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b80bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labels(\n",
    "    ds_dir: str,\n",
    "    yaml_rel_path: str,\n",
    "    index_map: dict[str, dict[int, int]],\n",
    "    images_path: str = IMAGES_PATH,\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the labels in the given dataset to use the global indices in `index_map`.\n",
    "\n",
    "    If this function raises an exception, then the dataset is likely corrupt!\n",
    "    \"\"\"\n",
    "    if ds_dir not in index_map:\n",
    "        raise ValueError(f\"'{ds_dir}' is not in the given index map.\")\n",
    "    \n",
    "    yaml_path = os.path.join(IMAGES_PATH, ds_dir, yaml_rel_path)\n",
    "    with open(yaml_path, \"r\") as yaml_file:\n",
    "        yaml_content = yaml.safe_load(yaml_file)\n",
    "\n",
    "    # Check if the labels have already been updated.\n",
    "    names = yaml_content[\"names\"]\n",
    "    if type(names) == dict:\n",
    "        if not names:\n",
    "            raise ValueError(\"No classes declared in dataset.\")\n",
    "\n",
    "        items = iter(names.items())\n",
    "        first_ident, first_name = next(items)\n",
    "        is_updated: bool = index_map[ds_dir][first_name] == first_ident\n",
    "\n",
    "        # Check the remaining identifiers and names for consistency.\n",
    "        for ident, name in items:\n",
    "            if (index_map[ds_dir][name] == ident) != is_updated:\n",
    "                raise ValueError(\"Detected partially updated dataset labels. The dataset is probably corrupt.\")\n",
    "    else:\n",
    "        # If the labels had been updated, the type of `names` would have been `dict`.\n",
    "        is_updated = False\n",
    "\n",
    "    if is_updated:\n",
    "        print(\"Dataset labels already updated — skipping\")\n",
    "    else:\n",
    "        _update_labels_yaml(ds_dir, yaml_rel_path, yaml_content, index_map, images_path)\n",
    "        _update_labels_txt(ds_dir, yaml_rel_path, yaml_content, index_map, images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_labels(\"coco-2017\", \"dataset.yaml\", index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc419a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_labels(\"guns\", \"data.yaml\", index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e4029-621b-4512-b64a-405ab62e9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_labels(\"knife\", \"data.yaml\", index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b04f1-e4f1-46a8-95f0-625639be01f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_labels(\"parcel\", \"data.yaml\", index_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50232d24",
   "metadata": {},
   "source": [
    "#### Create YAML File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557b353",
   "metadata": {},
   "source": [
    "Now that the labels have been updated, we can create a YAML file for the combined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813bc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_dataset_yaml(\n",
    "    output_ds_dir: str,\n",
    "    ds_yamls: list[tuple[str, str]],\n",
    "    index_map: dict[str, dict[int, int]],\n",
    "    images_path: str = IMAGES_PATH\n",
    "):\n",
    "    # Check if a YAML file already exists.\n",
    "    output_ds_dir_abs = os.path.join(images_path, output_ds_dir)\n",
    "    output_yaml_path = os.path.join(output_ds_dir_abs, \"dataset.yaml\")\n",
    "    if os.path.isfile(output_yaml_path):\n",
    "        print(f\"'{output_yaml_path}' exists — refusing to overwrite\")\n",
    "        return\n",
    "    \n",
    "    combined_yaml_content = {\n",
    "        \"train\": [],\n",
    "        \"val\": [],\n",
    "        \"test\": [],\n",
    "    }\n",
    "    \n",
    "    combined_yaml_content[\"names\"] = {\n",
    "        idx: name\n",
    "        for ds_index_map in index_map.values()\n",
    "        for name, idx in ds_index_map.items()\n",
    "        if type(name) == str # If not, then `name` refers to an old index, which we don't need here.\n",
    "    }\n",
    "    \n",
    "    combined_yaml_content[\"nc\"] = len(combined_yaml_content[\"names\"])\n",
    "\n",
    "    # Iterate the individual datasets.\n",
    "    for ds_dir, yaml_rel_path in ds_yamls:\n",
    "        yaml_path = os.path.join(images_path, ds_dir, yaml_rel_path)\n",
    "        with open(yaml_path, \"r\") as file:\n",
    "            yaml_content = yaml.safe_load(file)\n",
    "\n",
    "        for split in (\"train\", \"val\", \"test\"):\n",
    "            # In case the dataset is missing one of the splits.\n",
    "            if split not in yaml_content:\n",
    "                print(f\"Warning: dataset '{ds_dir}' does not contain '{split}' split — skipping split\")\n",
    "                continue\n",
    "\n",
    "            if type(yaml_content[split]) == str:\n",
    "                split_rel_paths = [yaml_content[split]]\n",
    "            elif type(yaml_content[split]) == list:\n",
    "                split_rel_paths = yaml_content[split]\n",
    "            else:\n",
    "                raise ValueError(f\"Encountered type '{type(yaml_content[split])}' for '{split}' field in '{yaml_path}' — don't know how to handle\")\n",
    "\n",
    "            # Parent directory of the YAML file, relative to `images_path`.\n",
    "            parent_yaml_path = os.path.dirname(os.path.join(ds_dir, yaml_rel_path))\n",
    "            split_paths = []\n",
    "            for split_rel_path in split_rel_paths:\n",
    "                # Path to the split, relative to `images_path`.\n",
    "                split_path = os.path.join(parent_yaml_path, split_rel_path)\n",
    "\n",
    "                # Path to the split, relative to the output directory\n",
    "                # Assumes the output directory will be directly under `images_path`.\n",
    "                split_path = os.path.join(\"..\", split_path)\n",
    "                split_paths.append(os.path.normpath(split_path))\n",
    "            \n",
    "            combined_yaml_content[split].extend(split_paths)\n",
    "\n",
    "    \n",
    "    # Create the output directory.\n",
    "    if not os.path.isdir(output_ds_dir_abs):\n",
    "        os.mkdir(output_ds_dir_abs)\n",
    "        \n",
    "    # Write combined YAML content out.\n",
    "    with open(output_yaml_path, \"w\") as file:\n",
    "        yaml.dump(combined_yaml_content, file)\n",
    "\n",
    "    print(f\"Successfully wrote dataset configuration to '{output_yaml_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64525627",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_combined_dataset_yaml(\n",
    "    COMBINED_DATASET_DIR,\n",
    "    [\n",
    "        (\"coco-2017\", \"dataset.yaml\"),\n",
    "        (\"guns\", \"data.yaml\"),\n",
    "        (\"knife\", \"data.yaml\"),\n",
    "        (\"parcel\", \"data.yaml\"),\n",
    "    ],\n",
    "    index_map\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb04b2",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3a30a",
   "metadata": {},
   "source": [
    "We'll perform hyperparameter tuning using Ultralytics' integration with Ray Tune. As part of this hyperparameter tuning, we also explore techniques and parameters for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8daf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725c65d-b3ee-4576-9869-4df7cbbb477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"lr0\": tune.uniform(1e-5, 1e-1),\n",
    "    \"lrf\": tune.uniform(0.01, 1.0),\n",
    "    \"momentum\": tune.uniform(0.6, 0.98),\n",
    "    \"weight_decay\": tune.uniform(0.0, 0.001),\n",
    "    \"warmup_epochs\": tune.uniform(0.0, 5.0),\n",
    "    \"warmup_momentum\": tune.uniform(0.0, 0.95),\n",
    "    \"box\": tune.uniform(0.02, 0.2),\n",
    "    \"cls\": tune.uniform(0.2, 4.0),\n",
    "    \"hsv_h\": tune.uniform(0.0, 0.2),\n",
    "    \"hsv_s\": tune.uniform(0.0, 0.9),\n",
    "    \"hsv_v\": tune.uniform(0.0, 0.9),\n",
    "    \"degrees\": tune.uniform(0.0, 90.0),\n",
    "    \"translate\": tune.uniform(0.0, 0.9),\n",
    "    \"scale\": tune.uniform(0.0, 0.9),\n",
    "    \"shear\": tune.uniform(0.0, 10.0),\n",
    "    \"perspective\": tune.uniform(0.0, 0.001),\n",
    "    \"flipud\": tune.uniform(0.0, 1.0),\n",
    "    \"fliplr\": tune.uniform(0.0, 1.0),\n",
    "    \"bgr\": tune.uniform(0.0, 1.0),\n",
    "    \"mosaic\": tune.uniform(0.0, 1.0),\n",
    "    \"mixup\": tune.uniform(0.0, 1.0),\n",
    "    \"copy_paste\": tune.uniform(0.0, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60df16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = model.tune(\n",
    "    data=os.path.join(IMAGES_PATH, COMBINED_DATASET_DIR, \"dataset.yaml\"),\n",
    "    use_ray=True,\n",
    "    space=space,\n",
    "    epochs=25,\n",
    "    grace_period=10,\n",
    "    gpu_per_trial=2, # Tweak according to how many GPUs you have.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.cfg import TASK2METRIC\n",
    "\n",
    "metric = TASK2METRIC[\"detect\"]\n",
    "print(f\"Retrieving best result with highest '{metric}'\")\n",
    "\n",
    "best_result = result_grid.get_best_result(metric=metric, mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8b1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513a848-b4c7-4c5d-81dc-c5e0a6f018b9",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ca00d-362f-4b09-aea3-e250a9281549",
   "metadata": {},
   "source": [
    "Finally, we can fine-tune the model using the best set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130e7d7-eb61-472f-b549-1808ac4e4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(model, yaml_path, epochs=5, imgsz=640, batch=16, device=None, patience=5, **train_kwargs):\n",
    "    # Prepare the arguments for model.train().\n",
    "    default_train_kwargs = {\n",
    "        'data': yaml_path,\n",
    "        'epochs': epochs, \n",
    "        'imgsz': imgsz,\n",
    "        'batch': batch,\n",
    "        'patience': patience,\n",
    "    }\n",
    "\n",
    "    # Include `device` only if it is specified.\n",
    "    if device is not None: \n",
    "        train_kwargs['device'] = device\n",
    "        \n",
    "    # Merge other arguments.\n",
    "    train_kwargs = default_train_kwargs | train_kwargs\n",
    "\n",
    "    print(f\"Train arguments: {train_kwargs}\\n\")\n",
    "\n",
    "    model.train(**train_kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa14cb-db9f-426a-b16b-a30cee24a640",
   "metadata": {},
   "source": [
    "Use the hyperparameters determined by hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20785578-a300-4fc7-986d-f4d8cd392bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with dictionary of best parameters from hyperparameter tuning (`best_result.config`).\n",
    "train_kwargs = best_result.config.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45b2df-69df-47aa-bf4e-f06e8883bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove values that shouldn't be in there when passing to the fine-tune function.\n",
    "if \"epochs\" in train_kwargs:\n",
    "    del train_kwargs[\"epochs\"]\n",
    "    \n",
    "if \"data\" in train_kwargs:\n",
    "    del train_kwargs[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2835a15-79be-4467-9430-697aa9e911db",
   "metadata": {},
   "source": [
    "Although we specify 500 epochs, the process will automatically stop when performance stops improving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc5561-ee4a-4760-b3b2-f506c44debed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune the YOLO model with the combined dataset.\n",
    "model = fine_tune(\n",
    "    model,\n",
    "    os.path.join(IMAGES_PATH, COMBINED_DATASET_DIR, \"dataset.yaml\"),\n",
    "    epochs=500,\n",
    "    patience=25,\n",
    "    **train_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013c418-3a40-47de-adb6-da1bbd79a3dd",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c83e1a-05b0-4893-9a43-6e91d84016fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befd6bdf-9568-4296-9487-c25fdc23a4fa",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da75741-fc96-41c3-9dcc-9291b70963f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8cff-b7e8-4de6-b6cb-4688c50181ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.val(data=os.path.join(IMAGES_PATH, COMBINED_DATASET_DIR, \"dataset.yaml\"), split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10e8b0-3f3d-4574-93a3-9cc180d02c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_results(results):\n",
    "    print(\"Per-class metrics:\")\n",
    "    print(f\"  Class indices: {results.ap_class_index}\")\n",
    "    print(f\"  Precision for each class: {results.box.p}\")\n",
    "    print(f\"  Recall for each class: {results.box.r}\")\n",
    "    print(f\"  F1 score for each class: {results.box.f1}\")\n",
    "\n",
    "    # Mean results\n",
    "    print(f\"Mean precision: {results.box.mp}\")\n",
    "    print(f\"Mean recall: {results.box.mr}\")\n",
    "    \n",
    "    # Mean average precision (mAP)\n",
    "    print(f\"Mean average precision at IoU=0.50 to 0.95 (mAP50-95): {results.box.map}\")\n",
    "    print(f\"Mean average precision at IoU=0.50 (mAP50): {results.box.map50}\")\n",
    "    print(f\"Mean average precision at IoU=0.75 (mAP75): {results.box.map75}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5b39a-e7af-49d9-915d-944b667996d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation_results(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
