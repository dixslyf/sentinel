{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning YOLOv8 with External Dataset and Exporting to TFLite\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import shutil\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from roboflow import Roboflow\n",
    "from dotenv import load_dotenv\n",
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Pre-Trained YOLOv8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Roboflow dataset\n",
    "\n",
    "To use this section, create `.env` file and add the following: \n",
    "``` bash\n",
    "ROBOFLOW_API=<your_api_key>\n",
    "```\n",
    "\n",
    "**Notes**:\n",
    "Make sure that the `.env` file exists with the correct variables if not the code cell below will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "FOLDER_PATH = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "DATAFOLDER = os.path.join(FOLDER_PATH, 'sentinel_model_gen', 'data-images')\n",
    "\n",
    "# load roboflow api from environment\n",
    "load_dotenv()\n",
    "rf_api = os.getenv('ROBOFLOW_API')\n",
    "rf = Roboflow(api_key=os.getenv('ROBOFLOW_API'))\n",
    "\n",
    "# function to download data from roboflow\n",
    "def download_data(workspace, project, folder_name='Unknown'):\n",
    "    project = rf.workspace(workspace).project(project)\n",
    "    version = project.version(2)\n",
    "    dataset = version.download(\"yolov8\", location=os.path.join(DATAFOLDER, folder_name))\n",
    "\n",
    "# Download gun images from roboflow\n",
    "download_data(\"liteye-systems\", \"weapon-classification\", \"gun-images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Coco Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_export_coco(classes, split_name=None, max_samples=5000):\n",
    "    # load the dataset split \n",
    "    dataset = fo.zoo.load_zoo_dataset(\n",
    "        \"coco-2017\",\n",
    "        # split=split_name,\n",
    "        label_types=[\"detections\"],\n",
    "        classes=classes,\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "\n",
    "    export_dir = os.path.join(DATAFOLDER, \"coco\")\n",
    "\n",
    "    if not os.path.exists(export_dir):\n",
    "        os.makedirs(export_dir)\n",
    "    \n",
    "    split_export_dir = os.path.join(export_dir, split_name)\n",
    "\n",
    "    # Export to YOLOv8\n",
    "    dataset.export(\n",
    "        export_dir=split_export_dir,\n",
    "        dataset_type=fo.types.YOLOv5Dataset, # YOLOv5 and YOLOv8 use the same format\n",
    "    )\n",
    "\n",
    "    print(f\"{split_name.capitalize()} split exported to {split_export_dir}\")\n",
    "\n",
    "# download_export_coco(\"train\", classes=[\"person\"], max_samples=10000)\n",
    "# download_export_coco(\"validation\", classes=[\"person\"], max_samples=10000)\n",
    "# download_export_coco(\"test\", classes=[\"person\"], max_samples=10000)\n",
    "\n",
    "# this is without specifying split names\n",
    "#download_export_coco(classes=['person'], max_samples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip dataset zip file (Not used anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data files into the specified folder\n",
    "def unzip_data(zip_file, folder_path):\n",
    "    # created folder if it does not exist\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # unzip the contents of the zip file to the destination folder\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(folder_path)\n",
    "\n",
    "    print(f\"{zip_file} unzip to {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined folder for datasets \n",
    "COMBINED_FOLDER = os.path.join(DATAFOLDER, 'combined-images')\n",
    "\n",
    "if not os.path.exists(COMBINED_FOLDER):\n",
    "    os.makedirs(COMBINED_FOLDER)\n",
    "\n",
    "def combine_and_rename(src_folder, dataset_name, class_offset):\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        img_src_folder = os.path.join(src_folder, split, 'images')\n",
    "        lbl_src_folder = os.path.join(src_folder, split, 'labels')\n",
    "\n",
    "        img_dest_folder = os.path.join(COMBINED_FOLDER, split, 'images')\n",
    "        lbl_dest_folder = os.path.join(COMBINED_FOLDER, split, 'labels')\n",
    "\n",
    "        if not os.path.exists(img_dest_folder):\n",
    "            os.makedirs(img_dest_folder)\n",
    "        if not os.path.exists(lbl_dest_folder):\n",
    "            os.makedirs(lbl_dest_folder)\n",
    "\n",
    "        img_files = sorted(os.listdir(img_src_folder))\n",
    "        lbl_files = sorted(os.listdir(lbl_src_folder))\n",
    "\n",
    "        for i, img_file in enumerate(img_files):\n",
    "            lbl_file = lbl_files[i]\n",
    "\n",
    "            new_img_name = f\"{dataset_name}-img-{i+1}.jpg\"\n",
    "            new_lbl_name = f\"{dataset_name}-img-{i+1}.txt\"\n",
    "\n",
    "            img_src_path = os.path.join(img_src_folder, img_file)\n",
    "            lbl_src_path = os.path.join(lbl_src_folder, lbl_file)\n",
    "\n",
    "            img_dest_path = os.path.join(img_dest_folder, new_img_name)\n",
    "            lbl_dest_path = os.path.join(lbl_dest_folder, new_lbl_name)\n",
    "\n",
    "            # Read and modify label file content (adjust class IDs)\n",
    "            with open(lbl_src_path, 'r') as lbl_file:\n",
    "                lines = lbl_file.readlines()\n",
    "\n",
    "            with open(lbl_dest_path, 'w') as new_lbl_file:\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    class_id = int(parts[0]) + class_offset\n",
    "                    new_line = f\"{class_id} \" + \" \".join(parts[1:]) + \"\\n\"\n",
    "                    new_lbl_file.write(new_line)\n",
    "\n",
    "            # Copy the image file\n",
    "            shutil.copy(img_src_path, img_dest_path)\n",
    "\n",
    "            # Augmentation logic\n",
    "            # Only augment underrepresented classes based on `class_occurrences`\n",
    "            # with open(lbl_src_path, 'r') as lbl_file:\n",
    "            #     for line in lbl_file.readlines():\n",
    "            #         class_id = int(line.split()[0]) + class_offset\n",
    "            #         # If the class ID has fewer samples, augment the image\n",
    "            #         if class_occurrences[class_id] < 500:  # Define threshold for underrepresented classes\n",
    "            #             augment_image(img_src_path, lbl_src_path, img_dest_folder, i, dataset_name)\n",
    "\n",
    "            #print(f\"Moved {img_file} -> {new_img_name} with updated labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create yaml file for combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(COMBINED_FOLDER, 'train', 'images')\n",
    "VAL_PATH = os.path.join(COMBINED_FOLDER, 'valid', 'images')\n",
    "TEST_PATH = os.path.join(COMBINED_FOLDER, 'test', 'images')\n",
    "OUTPUT_PATH = os.path.join(COMBINED_FOLDER, 'data.yaml')\n",
    "\n",
    "\n",
    "def load_yaml(yaml_path):\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def combine_yaml_new(yaml_files, class_offsets):\n",
    "    combined_data = {\n",
    "        'train': TRAIN_PATH,\n",
    "        'val': VAL_PATH,\n",
    "        'test': TEST_PATH,\n",
    "        'names': []  # will hold class names at the right index positions\n",
    "    }\n",
    "\n",
    "    # Loop through each yaml file and its corresponding class offset\n",
    "    for i, yaml_file in enumerate(yaml_files):\n",
    "        data = load_yaml(yaml_file)\n",
    "        offset = class_offsets[i]  # get the offset for the current dataset\n",
    "\n",
    "        # Ensure combined_data['names'] list is large enough\n",
    "        max_index = offset + len(data['names']) - 1\n",
    "        while len(combined_data['names']) <= max_index:\n",
    "            combined_data['names'].append(None)\n",
    "\n",
    "        # Insert class names at the correct index positions\n",
    "        for j, class_name in enumerate(data['names']):\n",
    "            combined_data['names'][offset + j] = class_name\n",
    "\n",
    "    # Fill any None entries with default values (optional, in case something was missed)\n",
    "    combined_data['names'] = [name if name is not None else 'unknown' for name in combined_data['names']]\n",
    "\n",
    "    combined_data['nc'] = len(combined_data['names'])  # set the number of unique classes\n",
    "\n",
    "    # Write combined data to a new YAML file\n",
    "    with open(OUTPUT_PATH, 'w+') as yaml_f:\n",
    "        yaml.dump(combined_data, yaml_f)\n",
    "\n",
    "    print(f\"Combined YAML file created at {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Data Imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_class_occurrences(folder_path):\n",
    "    label_files = []\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        lbl_folder = os.path.join(folder_path, split, 'labels')\n",
    "        for file in os.listdir(lbl_folder):\n",
    "            label_files.append(os.path.join(lbl_folder, file))\n",
    "\n",
    "    class_counter = Counter()\n",
    "    for lbl_file in label_files:\n",
    "        with open(lbl_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                class_id = int(line.split()[0])\n",
    "                class_counter[class_id] += 1\n",
    "\n",
    "    print(f\"Class Occurrences: {class_counter}\")\n",
    "    return class_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Imbalanced Data (Does not work right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation pipeline\n",
    "AUGMENTATIONS = [\n",
    "    A.HorizontalFlip(p=1.0),\n",
    "    A.Rotate(limit=20, p=1.0),\n",
    "    A.RandomBrightnessContrast(p=1.0),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, p=1.0)\n",
    "]\n",
    "\n",
    "# Augment image function\n",
    "def augment_image(image_path, label_path, aug_folder, index, dataset_name):\n",
    "    image = cv2.imread(image_path)\n",
    "    img_height, img_width = image.shape[:2]\n",
    "\n",
    "    # Read bounding box labels\n",
    "    with open(label_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for i, aug in enumerate(AUGMENTATIONS):\n",
    "        augmented = aug(image=image)\n",
    "        aug_image = augmented['image']\n",
    "\n",
    "        # Save the augmented image with a new name\n",
    "        aug_img_name = f\"{dataset_name}-img-{index+1}-aug-{i+1}.jpg\"\n",
    "        aug_img_path = os.path.join(aug_folder, aug_img_name)\n",
    "        cv2.imwrite(aug_img_path, aug_image)\n",
    "\n",
    "        # Save the corresponding augmented label file\n",
    "        aug_lbl_name = f\"{dataset_name}-img-{index+1}-aug-{i+1}.txt\"\n",
    "        aug_lbl_path = os.path.join(aug_folder, aug_lbl_name)\n",
    "        with open(aug_lbl_path, 'w') as aug_label_file:\n",
    "            aug_label_file.writelines(lines)\n",
    "\n",
    "    print(f\"Augmented image and label saved to {aug_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune the YOLO model with new dataset\n",
    "# epochs=5 for testing purposes\n",
    "def fine_tune(model, yaml_path, epochs=5, imgsz=640, batch=16, device=None):\n",
    "    # model.train(data=yaml_path, epochs=epochs, imgsz=imgsz, batch=batch)\n",
    "    # prepare the arguments for model.train\n",
    "    train_kwargs = {\n",
    "        'data': yaml_path,\n",
    "        'epochs': epochs, \n",
    "        'imgsz': imgsz,\n",
    "        'batch': batch\n",
    "    }\n",
    "\n",
    "    # include 'deivce' only if its not none \n",
    "    if device is not None: \n",
    "        train_kwargs['device'] = device\n",
    "\n",
    "    # train model \n",
    "    model.train(**train_kwargs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "def save_model(model):\n",
    "    model.save('yolo_fine_tuned.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets (Will create a new section for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip parcel images dataset into the 'parcel-images' folder\n",
    "unzip_data(os.path.join(DATAFOLDER, \"parcel.v1i.yolov8.zip\"), os.path.join(DATAFOLDER, \"parcel-images\"))\n",
    "\n",
    "# unzip the gun images dataset into the 'gun-images' folder\n",
    "unzip_data(os.path.join(DATAFOLDER, \"Weapon classification.v2i.yolov8.zip\"), os.path.join(DATAFOLDER, \"gun-images\"))\n",
    "\n",
    "# unzip the human images dataset into the 'human-images' folder\n",
    "unzip_data(os.path.join(DATAFOLDER, \"Crowd Detection.v3i.yolov8.zip\"), os.path.join(DATAFOLDER, \"human-images\"))\n",
    "\n",
    "# dataset source folder \n",
    "parcel_folder = os.path.join(DATAFOLDER, \"parcel-images\")\n",
    "gun_folder = os.path.join(DATAFOLDER, \"gun-images\")\n",
    "human_folder = os.path.join(DATAFOLDER, \"human-images\")\n",
    "\n",
    "# Calculate class occurrences \n",
    "# class_occurrences = count_class_occurrences(COMBINED_FOLDER)\n",
    "# print(f\"Before balancing data: {class_occurrences}\")\n",
    "\n",
    "# combine dataset with augmentation for underrepresented classes\n",
    "combine_and_rename(parcel_folder, 'parcel', class_offset=0)\n",
    "combine_and_rename(gun_folder, 'gun', class_offset=1)\n",
    "combine_and_rename(human_folder, 'human', class_offset=3)\n",
    "\n",
    "# print(f\"After balancing data: {count_class_occurrences(COMBINED_FOLDER)}\")\n",
    "\n",
    "# path to yaml configuration for parcel images and gun images\n",
    "parcel_yaml = os.path.join(DATAFOLDER, \"parcel-images\", \"data.yaml\")\n",
    "gun_yaml = os.path.join(DATAFOLDER, \"gun-images\", \"data.yaml\")\n",
    "human_yaml = os.path.join(DATAFOLDER, \"human-images\", \"data.yaml\")\n",
    "\n",
    "class_offsets = [0, 1, 3]\n",
    "yaml_list = [parcel_yaml, gun_yaml, human_yaml]\n",
    "\n",
    "combine_yaml_new(yaml_list, class_offsets)\n",
    "\n",
    "count_class_occurrences(COMBINED_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataset Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data source folder \n",
    "gun_folder = os.path.join(DATAFOLDER, \"gun-images\")\n",
    "\n",
    "# combine dataset \n",
    "combine_and_rename(gun_folder, 'gun', class_offset=0)\n",
    "\n",
    "gun_yaml = os.path.join(DATAFOLDER, \"gun-images\", \"data.yaml\")\n",
    "\n",
    "class_offsets = [0]\n",
    "yaml_list = [gun_yaml]\n",
    "\n",
    "combine_yaml_new(yaml_list, class_offsets)\n",
    "\n",
    "count_class_occurrences(COMBINED_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune the YOLO model with combined dataset images \n",
    "fined_tuned_model = fine_tune(model, os.path.join(COMBINED_FOLDER, \"data.yaml\"))\n",
    "\n",
    "#fined_tuned_model = fine_tune(model, gun_yaml)\n",
    "\n",
    "# save the model\n",
    "save_model(fined_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = YOLO(model_name)\n",
    "    return model\n",
    "\n",
    "fined_tuned_model = load_model('yolo_fine_tuned.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def test_model(model, img_path, conf=0.25):\n",
    "    # Perform object detection\n",
    "    results = model(img_path, conf=conf)\n",
    "\n",
    "    # retrieve the annotated image (with bounding boxes and labels)\n",
    "    annotated_img = results[0].plot()\n",
    "\n",
    "    # display the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(annotated_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_model(fined_tuned_model, os.path.join(FOLDER_PATH, 'sentinel_model_gen', 'guy_w_box.png'))\n",
    "test_model(fined_tuned_model, os.path.join(FOLDER_PATH, 'sentinel_model_gen', 'guy.png'))\n",
    "test_model(fined_tuned_model, os.path.join(FOLDER_PATH, 'sentinel_model_gen', 'guy_w_gun.png'))\n",
    "test_model(fined_tuned_model, os.path.join(FOLDER_PATH, 'sentinel_model_gen', 'guy_w_gun_2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model to TFLite Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model to TFLite for use in the detection system \n",
    "def export_model(model): \n",
    "    model.export(format='tflite')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
